{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5ac87e",
   "metadata": {},
   "source": [
    "# Tecnologías de Almacenamiento\n",
    "\n",
    "## Tema 5. Apache Spark. Spark SQL\n",
    "\n",
    "Este notebook incluye el código de ejemplo del manual del módulo\n",
    "\n",
    "Usamos el contenedor jupyter/all-spark-notebook\n",
    "```\n",
    "docker run -–name spark-stack -p 10000:8888 -p 4040:4040 jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "Ejecutamos con el kernel de Scala: Spylon-kernel\n",
    "\n",
    "(acg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f1c09",
   "metadata": {},
   "source": [
    "### 4.1 RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f984aced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cadenas: Array[String] = Array(master, bigdata, spark, data, master, uemc, uemc, master, bigdata)\n",
       "cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cadenas = Array(\"master\",\"bigdata\",\"spark\",\"data\", \"master\", \"uemc\", \"uemc\",\"master\",\"bigdata\")\n",
    "val cadenasRDD = sc.parallelize(cadenas, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c490cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: org.apache.spark.rdd.RDD[String] = ./car_samples.csv MapPartitionsRDD[2] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = sc.textFile(\"./car_samples.csv\", 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee37d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileNotFound: org.apache.spark.rdd.RDD[String] = /home/vmuser/fileNotFound MapPartitionsRDD[4] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileNotFound = sc.textFile(\"/home/vmuser/fileNotFound\", 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea7785a",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input path does not exist: file:/home/vmuser/fileNotFound",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vmuser/fileNotFound",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)",
      "  ... 38 elided",
      "Caused by: java.io.IOException: Input path does not exist: file:/home/vmuser/fileNotFound",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)",
      "  ... 54 more",
      ""
     ]
    }
   ],
   "source": [
    "fileNotFound.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d02cc8",
   "metadata": {},
   "source": [
    "### 4.2 Transformaciones y acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc6f1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mayusculaRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at map at <console>:24\n",
       "res1: Array[String] = Array(MASTER, BIGDATA, SPARK, DATA, MASTER, UEMC, UEMC, MASTER, BIGDATA)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mayusculaRDD = cadenasRDD.map(p => p.toUpperCase()); mayusculaRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934166eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtrado: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at filter at <console>:24\n",
       "res2: Array[String] = Array(BIGDATA, DATA, BIGDATA)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtrado = mayusculaRDD.filter(p => p.contains(\"DATA\")) \n",
    "filtrado.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163cee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mayuscLength: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[7] at flatMap at <console>:24\n",
       "res3: Array[Any] = Array(MASTER, 6, BIGDATA, 7, SPARK, 5, DATA, 4, MASTER, 6, UEMC, 4, UEMC, 4, MASTER, 6, BIGDATA, 7)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mayuscLength = cadenasRDD.flatMap(p => List(p.toUpperCase(), p.length))\n",
    "mayuscLength.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f95f33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cadenasBoth: org.apache.spark.rdd.RDD[String] = UnionRDD[8] at union at <console>:25\n",
       "res4: Array[String] = Array(master, bigdata, spark, data, master, uemc, uemc, master, bigdata, MASTER, BIGDATA, SPARK, DATA, MASTER, UEMC, UEMC, MASTER, BIGDATA)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cadenasBoth = cadenasRDD.union(mayusculaRDD)\n",
    "cadenasBoth.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83bff525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distinct: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at distinct at <console>:25\n",
       "res6: Array[String] = Array(spark, master, data, bigdata, uemc)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distinct = cadenasRDD.distinct(1)\n",
    "distinct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "493beb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[15] at map at <console>:24\n",
       "res7: Array[(String, Int)] = Array((master,1), (bigdata,1), (spark,1), (data,1), (master,1), (uemc,1), (uemc,1), (master,1), (bigdata,1))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair = cadenasRDD.map(p => (p,1)) \n",
    "pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ac7019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[16] at groupByKey at <console>:24\n",
       "res8: Array[(String, Iterable[Int])] = Array((master,CompactBuffer(1, 1, 1)), (uemc,CompactBuffer(1, 1)), (spark,CompactBuffer(1)), (data,CompactBuffer(1)), (bigdata,CompactBuffer(1, 1)))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val group = pair.groupByKey() \n",
    "group.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124182e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suma: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at reduceByKey at <console>:24\n",
       "res9: Array[(String, Int)] = Array((master,3), (uemc,2), (spark,1), (data,1), (bigdata,2))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val suma = pair.reduceByKey(_ + _) \n",
    "suma.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7962064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[18] at map at <console>:25\n",
       "sort: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at sortByKey at <console>:27\n",
       "res10: Array[(String, Int)] = Array((uemc,1), (uemc,1), (spark,1), (master,1), (master,1), (master,1), (data,1), (bigdata,1), (bigdata,1))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair = cadenasRDD.map(p => (p,1))\n",
    "\n",
    "val sort = pair.sortByKey(false) \n",
    "sort.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e653565",
   "metadata": {},
   "source": [
    "#### Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f6e3869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeros: Array[Int] = Array(7, 8, 3, 6, 2, 1, 1)\n",
       "numerosRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[25] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numeros = Array(7,8,3,6,2,1,1)\n",
    "val numerosRDD = sc.parallelize(numeros, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e104728d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Int = 28\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerosRDD.reduce(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cde18bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Long = 16\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerosRDD.count() + cadenasRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32ab0464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: String = master\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadenasRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d4514e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[String] = Array(master, bigdata, spark, data)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadenasRDD.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52ef331c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Int = 8\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerosRDD.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "591e81d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[26] at map at <console>:25\n",
       "res23: scala.collection.Map[String,Long] = Map(bigdata -> 2, master -> 3, data -> 1, spark -> 1, uemc -> 2)\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair = cadenasRDD.map(p => (p,1)) \n",
    "pair.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecdc9a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra es data\n",
      "La palabra es master\n",
      "La palabra es uemc\n",
      "La palabra es master\n",
      "La palabra es bigdata\n",
      "La palabra es uemc\n",
      "La palabra es master\n",
      "La palabra es bigdata\n",
      "La palabra es spark\n"
     ]
    }
   ],
   "source": [
    "cadenasRDD.foreach(p => println(\"La palabra es \" + p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0815372",
   "metadata": {},
   "source": [
    "### 4.3 Datasets\n",
    "\n",
    "Este bloque en principio conviene hacerlo en el shell de spark del contenedor\n",
    "```\n",
    "docker exec -it spark-stack spark-shell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "364c393a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@37b3c514\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"miApp\").master(\"local\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fd7b0",
   "metadata": {},
   "source": [
    "recuerda subir el fichero de strangersCharacters.txt al contenedor:\n",
    "```\n",
    "docker cp strangersCharacters.txt spark-stack:/home/jovyan/strangersCharacters.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4383ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lobezno,125,hombre\r\n",
      "Catwoman,32,mujer\r\n",
      "Batman,43,hombre\r\n",
      "ScarletWitch,28,mujer\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat strangersCharacters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cd712be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@c7fc64f\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7edc643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineas: org.apache.spark.rdd.RDD[String] = strangersCharacters.txt MapPartitionsRDD[30] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lineas = sc.textFile(\"strangersCharacters.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ae83e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row \n",
    "\n",
    "import spark.implicits._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d111cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import spark.implicits._\n",
       "partes: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[31] at map at <console>:34\n",
       "res25: Array[Array[String]] = Array(Array(Lobezno, 125, hombre), Array(Catwoman, 32, mujer), Array(Batman, 43, hombre), Array(ScarletWitch, 28, mujer))\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row \n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val partes = lineas.map(_.split(\",\")) \n",
    "\n",
    "partes.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff79fa",
   "metadata": {},
   "source": [
    "El siguiente bloque en el contenedor no funciona, ejecutadlo en el shell de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "012184cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "//case class Personaje(nombre: String, edad: Long, sexo: String)\n",
    "//val personajes = partes.map(atr => Personaje(atr(0), atr(1).trim.toInt, atr(2))).toDS()\n",
    "//personajes.select($\"nombre\").first\n",
    "//personajes.show() \n",
    "//personajes.foreach(p => println(\"El personaje \" + p.nombre + \" tiene \" + p.edad + \" años\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9560af",
   "metadata": {},
   "source": [
    "### 4.4 Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37993796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@37b3c514\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"miApp\").master(\"local\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71422a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Nombre,StringType,true), StructField(Edad,IntegerType,true), StructField(Genero,StringType,true))\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schema = new StructType()\n",
    "      .add(\"Nombre\",StringType,true)\n",
    "      .add(\"Edad\",IntegerType,true)\n",
    "      .add(\"Genero\",StringType,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "957cecfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "characters: org.apache.spark.sql.DataFrame = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val characters = spark.read.format(\"csv\").option(\"header\", \"false\").schema(schema).load(\"strangersCharacters.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "195d77c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|      Nombre|Edad|Genero|\n",
      "+------------+----+------+\n",
      "|     Lobezno| 125|hombre|\n",
      "|    Catwoman|  32| mujer|\n",
      "|      Batman|  43|hombre|\n",
      "|ScarletWitch|  28| mujer|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "515fce1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Array[String] = Array(Nombre, Edad, Genero)\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "86acde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|      nombre|edad|\n",
      "+------------+----+\n",
      "|     Lobezno| 125|\n",
      "|    Catwoman|  32|\n",
      "|      Batman|  43|\n",
      "|ScarletWitch|  28|\n",
      "+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters.select(\"nombre\",\"edad\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80ecd066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|      Nombre|Edad|Genero|\n",
      "+------------+----+------+\n",
      "|ScarletWitch|  28| mujer|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "youngs: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val youngs = characters.filter($\"edad\" < 30) \n",
    "youngs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15a12117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: org.apache.spark.sql.Row = [Lobezno,125,hombre]\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5215cb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tresPrimeros: Array[org.apache.spark.sql.Row] = Array([Lobezno,125,hombre], [Catwoman,32,mujer], [Batman,43,hombre])\n",
       "res42: org.apache.spark.sql.Row = [Catwoman,32,mujer]\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tresPrimeros = characters.head(3) \n",
    "tresPrimeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a7cac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Long = 4\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2d501c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|edad|count|\n",
      "+----+-----+\n",
      "|  28|    1|\n",
      "|  43|    1|\n",
      "| 125|    1|\n",
      "|  32|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters.groupBy(\"edad\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f3faa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------------+------+\n",
      "|summary|      Nombre|            Edad|Genero|\n",
      "+-------+------------+----------------+------+\n",
      "|  count|           4|               4|     4|\n",
      "|   mean|        null|            57.0|  null|\n",
      "| stddev|        null|45.7748111228581|  null|\n",
      "|    min|      Batman|              28|hombre|\n",
      "|    max|ScarletWitch|             125| mujer|\n",
      "+-------+------------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1d55e",
   "metadata": {},
   "source": [
    "### 4.5 Vistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e3c62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.createOrReplaceTempView(\"personajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5123f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|      nombre|edad|\n",
      "+------------+----+\n",
      "|    Catwoman|  32|\n",
      "|ScarletWitch|  28|\n",
      "+------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mujeres: org.apache.spark.sql.DataFrame = [nombre: string, edad: int]\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mujeres = spark.sql(\"select nombre,edad from personajes where Genero = 'mujer'\" )\n",
    "mujeres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbf28d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@72363a46\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = spark.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7bac1068",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Table or view not found: personajes; line 1 pos 14;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Table or view not found: personajes; line 1 pos 14;",
      "'Project [*]",
      "+- 'UnresolvedRelation [personajes], [], false",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:123)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:172)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:195)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:192)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)",
      "  ... 43 elided",
      ""
     ]
    }
   ],
   "source": [
    "val t = spark2.sql(\"select * from personajes\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97539b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters.createOrReplaceGlobalTempView(\"personajes_global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1adb5d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|      nombre|edad|\n",
      "+------------+----+\n",
      "|    Catwoman|  32|\n",
      "|ScarletWitch|  28|\n",
      "+------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t: org.apache.spark.sql.DataFrame = [nombre: string, edad: int]\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = spark2.sql(\"select nombre,edad from global_temp.personajes_global where Genero = 'mujer'\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b0d2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|genero|mean(edad)|\n",
      "+------+----------+\n",
      "|hombre|      84.0|\n",
      "| mujer|      30.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "media_edad: org.apache.spark.sql.DataFrame = [genero: string, mean(edad): double]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val media_edad = spark2.sql(\"select genero, mean(edad) from global_temp.personajes_global group by genero\")\n",
    "media_edad.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15347902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
