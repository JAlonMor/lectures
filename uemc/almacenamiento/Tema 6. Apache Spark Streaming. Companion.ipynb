{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd74022",
   "metadata": {},
   "source": [
    "# Tecnologías de Almacenamiento\n",
    "\n",
    "## Tema 6. Apache Spark. Spark Streaming\n",
    "\n",
    "Este notebook incluye el código de ejemplo del manual del módulo\n",
    "\n",
    "Usamos el contenedor jupyter/all-spark-notebook\n",
    "```\n",
    "docker run --name spark-stack -p 10000:8888 -p 4040:4040 jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "Ejecutamos con el kernel de Scala: Spylon-kernel\n",
    "\n",
    "Antes de empezar creamos un socket en el puerto 9999 de nuestro contenedor:\n",
    "```\n",
    "docker exec -it spark-stack nc -l 9999\n",
    "```\n",
    "No mates el proceso, se quedará bloqueado el shell\n",
    "\n",
    "(acg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b43c1",
   "metadata": {},
   "source": [
    "### 2.1 Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b534e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://16a66b851a51:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1636196821567)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row \n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b08ca38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineas: org.apache.spark.sql.DataFrame = [value: string]\n",
       "res0: Boolean = true\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lineas = spark.readStream\n",
    "                  .format(\"socket\")\n",
    "                  .option(\"host\", \"localhost\")\n",
    "                  .option(\"port\", 9999)\n",
    "                  .load() \n",
    "\n",
    "lineas.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f106a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lineas.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d7d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "palabras: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "numPalabras: org.apache.spark.sql.DataFrame = [value: string, count: bigint]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val palabras = lineas.as[String].flatMap(_.split(\" \"))\n",
    "\n",
    "val numPalabras = palabras.groupBy(\"value\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b571d65",
   "metadata": {},
   "source": [
    "La siguiente query se va a quedar escuchando (y refrescando resultados) durante 30 segundos (30000 msecs).\n",
    "\n",
    "Acuerdate de ir al shell donde lanzaste el netcat (nc) y escribir varias palabras:\n",
    "\n",
    "```\n",
    "(master_big_data) acg@MSI ~ $ docker exec -it spark-stack nc -l 9999\n",
    "Hola\n",
    "Hola otra vez\n",
    "donde esta la otra casa\n",
    "ve a casa\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc161dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "| Hola|    1|\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "| value|count|\n",
      "+------+-----+\n",
      "|  Hola|    2|\n",
      "|master|    1|\n",
      "|    es|    1|\n",
      "|    el|    1|\n",
      "|  data|    1|\n",
      "|  esto|    1|\n",
      "|   big|    1|\n",
      "+------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|   de|    1|\n",
      "| UEMC|    1|\n",
      "|   la|    1|\n",
      "+-----+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "| Hola|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@472b8ee3\n",
       "res3: Boolean = false\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = numPalabras.writeStream\n",
    "                       .outputMode(\"update\")\n",
    "                       .format(\"console\")\n",
    "                       .start() \n",
    "\n",
    "query.awaitTermination(30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d340c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944b24e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Boolean = false\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004eec5",
   "metadata": {},
   "source": [
    "Fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47658539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Nombre,StringType,true), StructField(Edad,IntegerType,true), StructField(Genero,StringType,true))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val schema = new StructType()\n",
    "      .add(\"Nombre\",StringType)\n",
    "      .add(\"Edad\",IntegerType)\n",
    "      .add(\"Genero\",StringType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d953f0f",
   "metadata": {},
   "source": [
    "Asegurate de subir el fichero de personajes a un **directorio**, el Stream debe recibir un directorio.\n",
    "\n",
    "Para hacerlo en nuestro contenedor de Docker\n",
    "```\n",
    "docker exec -it spark-stack mkdir todos_personajes\n",
    "docker cp ./strangersCharacters.txt spark-stack:/home/jovyan/todos_personajes/strangersCharacters.txt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b897fbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personasDF: org.apache.spark.sql.DataFrame = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personasDF = spark\n",
    "                .readStream\n",
    "                .schema(schema)\n",
    "                .csv(\"todos_personajes/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7eaf2",
   "metadata": {},
   "source": [
    "#### Sumideros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7806990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mujeres: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mujeres = personasDF.filter(\"Genero == 'mujer'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5395b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------+----+------+\n",
      "|      Nombre|Edad|Genero|\n",
      "+------------+----+------+\n",
      "|    Catwoman|  32| mujer|\n",
      "|ScarletWitch|  28| mujer|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@196d9c1\n",
       "res0: Boolean = false\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = mujeres.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .format(\"console\")\n",
    "  .start\n",
    "\n",
    "query.awaitTermination(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee7fff",
   "metadata": {},
   "source": [
    "Repetimos ahora con fichero json como sumidero para el filtro de hombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3041de64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personasDF: org.apache.spark.sql.DataFrame = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personasDF = spark\n",
    "                .readStream\n",
    "                .schema(schema)\n",
    "                .csv(\"todos_personajes/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55c51f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hombres: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Nombre: string, Edad: int ... 1 more field]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hombres = personasDF.filter(\"Genero == 'hombre'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e0f6609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@62bbe58c\n",
       "res3: Boolean = false\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = hombres.writeStream.outputMode(\"append\")\n",
    "            .format(\"json\")\n",
    "            .option(\"path\", \"./hombres_output\")\n",
    "            .option(\"checkpointLocation\", \"./hombres_output\")\n",
    "            .start() \n",
    "query.awaitTermination(30000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510d29",
   "metadata": {},
   "source": [
    "Veamos que ha generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc46896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits   part-00000-da91256b-cf28-43b2-80ca-390c28c9702f-c000.json\r\n",
      "metadata  sources\r\n",
      "offsets   _spark_metadata\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls hombres_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c679b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Nombre\":\"Lobezno\",\"Edad\":125,\"Genero\":\"hombre\"}\r\n",
      "{\"Nombre\":\"Batman\",\"Edad\":43,\"Genero\":\"hombre\"}\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat hombres_output/part-00000-da91256b-cf28-43b2-80ca-390c28c9702f-c000.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5714a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7906b013",
   "metadata": {},
   "source": [
    "### 2.2 Ventanas de tiempo y watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f7f079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.sql.Timestamp\n",
       "import org.apache.spark.sql.Row\n",
       "import spark.implicits._\n",
       "lineas: org.apache.spark.sql.DataFrame = [value: string, timestamp: timestamp]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.sql.Timestamp \n",
    "import org.apache.spark.sql.Row \n",
    "import spark.implicits._\n",
    "\n",
    "val lineas = spark.readStream\n",
    "                  .format(\"socket\")\n",
    "                  .option(\"host\", \"localhost\")\n",
    "                  .option(\"port\", 9999)\n",
    "                  .option(\"includeTimestamp\", true)\n",
    "                  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c3f3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "palabras: org.apache.spark.sql.DataFrame = [palabra: string, timestamp: timestamp]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val palabras = lineas.as[(String, Timestamp)].flatMap(line =>\n",
    "                                                        line._1.split(\" \").map(word => (word, line._2))\n",
    "                                                      ).toDF(\"palabra\", \"timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "796d2eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windowedCounts: org.apache.spark.sql.DataFrame = [window: struct<start: timestamp, end: timestamp>, palabra: string ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowedCounts = palabras.groupBy(window($\"timestamp\", \"10 seconds\", \"5 seconds\"), $\"palabra\")\n",
    "                             .count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd5452",
   "metadata": {},
   "source": [
    "Después de ejecutar el siguiente bloque (que se quedara ejecutando), vuelve a ir a la ventana del netcat a escribir palabras para enviarlas como streams a la query:\n",
    "```\n",
    "(master_big_data) acg@MSI ~ $ docker exec -it spark-stack nc -l 9999\n",
    "Hola\n",
    "Como estas\n",
    "donde estan las llaves\n",
    "estan en el fondo del mar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36e426f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-------+-----+\n",
      "|window|palabra|count|\n",
      "+------+-------+-----+\n",
      "+------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|Hola   |1    |\n",
      "|{2021-11-06 11:12:15, 2021-11-06 11:12:25}|Hola   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |palabra  |count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|es       |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|el       |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|streaming|1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|de       |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|modulo   |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|de       |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|stream   |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|del      |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|Este     |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|modulo   |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|stream   |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|es       |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|spark    |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|streaming|1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|Este     |1    |\n",
      "|{2021-11-06 11:12:25, 2021-11-06 11:12:35}|del      |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|el       |1    |\n",
      "|{2021-11-06 11:12:20, 2021-11-06 11:12:30}|spark    |1    |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------------+-----+\n",
      "|window                                    |palabra       |count|\n",
      "+------------------------------------------+--------------+-----+\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|asignatura    |1    |\n",
      "|{2021-11-06 11:12:30, 2021-11-06 11:12:40}|asignatura    |1    |\n",
      "|{2021-11-06 11:12:30, 2021-11-06 11:12:40}|almacenamiento|1    |\n",
      "|{2021-11-06 11:12:30, 2021-11-06 11:12:40}|la            |1    |\n",
      "|{2021-11-06 11:12:30, 2021-11-06 11:12:40}|de            |2    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|de            |2    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|la            |1    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|almacenamiento|1    |\n",
      "+------------------------------------------+--------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|big    |1    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|master |1    |\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|data   |1    |\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|de     |1    |\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|big    |1    |\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|master |1    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|de     |3    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|data   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|UEMC   |1    |\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|UEMC   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:12:40, 2021-11-06 11:12:50}|Hola   |1    |\n",
      "|{2021-11-06 11:12:35, 2021-11-06 11:12:45}|Hola   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4a65ba76\n",
       "res4: Boolean = false\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = windowedCounts.writeStream\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"truncate\", false)\n",
    "                          .format(\"console\")\n",
    "                          .start()\n",
    "\n",
    "query.awaitTermination(60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d41027",
   "metadata": {},
   "source": [
    "#### watermarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21a46cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "windowedCounts: org.apache.spark.sql.DataFrame = [window: struct<start: timestamp, end: timestamp>, palabra: string ... 1 more field]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowedCounts = palabras.withWatermark(\"timestamp\", \"15 seconds\")\n",
    "                             .groupBy(window($\"timestamp\", \"10 seconds\", \"5 seconds\"),$\"palabra\")\n",
    "                             .count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426615f",
   "metadata": {},
   "source": [
    "Recuerda de nuevo escribir palabras en el socket cuando ejecutes el siguiente bloque:\n",
    "```\n",
    "(master_big_data) acg@MSI ~ $ docker exec -it spark-stack nc -l 9999\n",
    "aaa\n",
    "bbb\n",
    "ccc\n",
    "bbb\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faf44b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1dcb6d5f\n",
       "res5: Boolean = false\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-------+-----+\n",
      "|window|palabra|count|\n",
      "+------+-------+-----+\n",
      "+------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|aaaa   |1    |\n",
      "|{2021-11-06 11:13:25, 2021-11-06 11:13:35}|aaaa   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|bbbb   |1    |\n",
      "|{2021-11-06 11:13:25, 2021-11-06 11:13:35}|bbbb   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|cccc   |1    |\n",
      "|{2021-11-06 11:13:25, 2021-11-06 11:13:35}|cccc   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|cccc   |2    |\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|Hola   |1    |\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|Hola   |1    |\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|cccc   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----+\n",
      "|window                                    |palabra |count|\n",
      "+------------------------------------------+--------+-----+\n",
      "|{2021-11-06 11:13:30, 2021-11-06 11:13:40}|llamando|1    |\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|llamando|1    |\n",
      "+------------------------------------------+--------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|otra   |1    |\n",
      "|{2021-11-06 11:13:40, 2021-11-06 11:13:50}|vez    |1    |\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|vez    |1    |\n",
      "|{2021-11-06 11:13:40, 2021-11-06 11:13:50}|Hola   |1    |\n",
      "|{2021-11-06 11:13:35, 2021-11-06 11:13:45}|Hola   |2    |\n",
      "|{2021-11-06 11:13:40, 2021-11-06 11:13:50}|otra   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val query = windowedCounts.writeStream\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"truncate\", false)\n",
    "                          .format(\"console\")\n",
    "                          .start() \n",
    "\n",
    "query.awaitTermination(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6002573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f4106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224e8310",
   "metadata": {},
   "source": [
    "### 2.3 Spark Streaming y DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481fea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.streaming._\n",
       "import org.apache.spark.streaming.StreamingContext._\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61e9a73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@c0d0110\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d831252",
   "metadata": {},
   "source": [
    "De nuevo, una vez lanzado el comando, escribimos palabras en el socket:\n",
    "```\n",
    "(master_big_data) acg@MSI ~ $ docker exec -it spark-stack nc -l 9999\n",
    "aaaa\n",
    "bbbb\n",
    "cccc\n",
    "Hola\n",
    "Vamos\n",
    "Master de Big Data\n",
    "Master de Movilidad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdea6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "val words = lines.flatMap(_.split(\" \"))\n",
    "\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((a,b) => a+b) \n",
    "\n",
    "wordCounts.print()\n",
    "\n",
    "ssc.start() \n",
    "ssc.awaitTerminationOrTimeout(45000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0605c",
   "metadata": {},
   "source": [
    "\n",
    "Si hemos iniciado el streaming arriba (ssc.start()), ya no nos va a dejar añadir este nuevo calculo, se puede volver a reiniciar el contexto y ejecutar ahora este bloque en vez del anterior (la principal differencia es el watermark de este en 15 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5723a7a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:14:50, 2021-11-06 11:15:00}|Hola   |1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|Hola   |1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1636197300000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|socket |1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|el     |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|el     |1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|socket |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|nuevo  |1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|en     |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|en     |1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|mensaje|1    |\n",
      "|{2021-11-06 11:14:55, 2021-11-06 11:15:05}|nuevo  |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|mensaje|1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+\n",
      "|window                                    |palabra|count|\n",
      "+------------------------------------------+-------+-----+\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|otro   |1    |\n",
      "|{2021-11-06 11:15:05, 2021-11-06 11:15:15}|mas    |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|mas    |1    |\n",
      "|{2021-11-06 11:15:05, 2021-11-06 11:15:15}|otro   |1    |\n",
      "|{2021-11-06 11:15:00, 2021-11-06 11:15:10}|mensaje|2    |\n",
      "|{2021-11-06 11:15:05, 2021-11-06 11:15:15}|mensaje|1    |\n",
      "+------------------------------------------+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1636197315000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1636197330000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1636197345000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@2c7566bb\n",
       "linesWindow: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.WindowedDStream@47b00120\n",
       "words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@1dbe78bd\n",
       "pairs: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.MappedDStream@7f1511d8\n",
       "wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@4c97e228\n",
       "res6: Boolean = false\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1636197360000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1636197375000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "val linesWindow = lines.window(Seconds(15))\n",
    "\n",
    "val words = linesWindow.flatMap(_.split(\" \"))\n",
    "\n",
    "val pairs = words.map(word => (word, 1))\n",
    "val wordCounts = pairs.reduceByKey((a,b) => a+b) \n",
    "wordCounts.print()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26148016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634a379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d906b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
